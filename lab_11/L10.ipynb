{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7964da77-8746-458d-b1c7-ae8655443bb6",
   "metadata": {},
   "source": [
    "# **Laboratory 8: Neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2695461-1fcc-4c7a-95d1-68f216478635",
   "metadata": {},
   "source": [
    "In this laboratory work, neural networks are introduced, and a type of network called a **multilayer perceptron (MLP) will be implemented.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1f80882-4fb7-4d66-8a57-ad2354d1ab96",
   "metadata": {},
   "source": [
    "A neural network is a non-linear computational model inspired by the human brain, designed to recognize patterns and learn from data. In its simplest form, a neural network consists of interconnected \"neurons\" organized in layers. Each neuron processes inputs to produce an output, which it transmits to the next layer, thus allowing the network to learn complex representations.\n",
    "\n",
    "The components of a neural network are:\n",
    "\n",
    "1. **Neuron (Perceptron)**: The fundamental element of a neural network that calculates a linear combination of input features based on weights $w$ and bias $b$:\n",
    "$$\n",
    "  z = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "2. **Activation Function**: Adds non-linearity, allowing neural networks to learn complex relationships. The most commonly used functions are:\n",
    "\n",
    "\n",
    "- ReLU (Rectified Linear Unit)\n",
    "$$ f(z) = max(z, 0)$$\n",
    "\n",
    "- Sigmoid: This function takes a real number and compresses it into an interval between 0 and 1, so large negative numbers become 0, and large positive numbers become 1. It can also be used in binary classification to transform scores into probabilities.\n",
    "\n",
    "$$ f(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/sigmoid.png\" alt=\"image\" width=\"200\" title=\"Neuron\"/>\n",
    "</p> \n",
    "<p align=\"center\"><em>Figure 1. The sigmoid function. </em></p>\n",
    "\n",
    "- Softmax (for multi-class classification): This function is most often used to transform the scores corresponding to the classes into a probability distribution.\n",
    "\n",
    "$$ f(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}} $$\n",
    "\n",
    "where K represents the number of classes, and $z_i$ is the score for class $i$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/neuron.png\" alt=\"image\" width=\"300\" title=\"Neuron\"/>\n",
    "</p> \n",
    "<p align=\"center\"><em>Figure 2. The mathematical model of a neuron. </em></p>\n",
    "\n",
    "\n",
    "Neural networks are modeled as groups of neurons connected in an acyclic graph. In other words, the outputs of some neurons can become inputs for other neurons. Neural network models are often organized into layers of neurons. For standard neural networks, the most common type of layer is the **fully connected layer**, in which all neurons between two adjacent layers are connected to each other,\n",
    "and neurons within a single layer have no connections between them. We define three main types of layers:\n",
    "\n",
    "- **Input Layer**: Receives the raw data, with each node representing an input feature.\n",
    "- **Hidden Layers**: Intermediate layers that learn to extract more complex features.\n",
    "- **Output Layer**: Provides the final prediction, organized differently depending on the task (e.g., one node for binary classification or multiple nodes for multi-class classification).\n",
    "\n",
    "\n",
    "Below are two examples of neural network topologies that use fully connected layers:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/mlp.png\" alt=\"image\" width=\"600\" title=\"Neuron\"/>\n",
    "</p> \n",
    "<p align=\"center\"><em>Figure 3. Left, a network with 2 layers (one hidden layer and one output layer). Right, a network with 3 layers (two hidden layers and one output layer). These networks use fully connected layers, where each neuron is connected to every neuron in the next layer. </em></p>\n",
    "\n",
    "\n",
    "### Multilayer Perceptrons (MLPs)\n",
    "A multilayer perceptron (MLP) is a type of neural network consisting of an input layer, one or more hidden layers, and an output layer. Each layer in the MLP is fully connected, meaning that each neuron in one layer is connected to every neuron in the next layer.\n",
    "\n",
    "#### Using MLPs for Classification\n",
    "\n",
    "##### **1. General Concepts**\n",
    "In classification, the goal of the MLP is to divide the data into distinct classes. In the case of binary classification, the output layer will have a single neuron that uses the sigmoid activation function. Thus, the network's output represents the probability of class 1:\n",
    "\n",
    "$$\n",
    "p(y = 1 | \\mathbf{x}; \\boldsymbol{\\theta}) = \\sigma(a) = \\frac{1}{1 + e^{-a}}\n",
    "$$\n",
    "\n",
    "where $y$ represents the class, $x$ are the network's input data, $\\theta$ are the model's parameters, $\\sigma$ represents the sigmoid function, and $a$ represents the **logits**, i.e., the output of the last neuron before applying the sigmoid activation function.\n",
    "\n",
    "In the case of multi-class classification, the network's output layer will have one neuron per class that uses the softmax activation function.\n",
    "\n",
    "A multilayer perceptron (MLP) tries to find an optimal non-linear decision surface using the *training* data. Thus, through the training process, the model adjusts its parameters (weights and biases) to learn to correctly separate the training data into the corresponding classes. The goal is to obtain a model that can classify new data from a test set with the highest possible accuracy.\n",
    "\n",
    "\n",
    "##### **2. Splitting the Dataset**\n",
    "Splitting the dataset into training, validation, and test subsets is an essential practice in training neural networks.\n",
    "\n",
    "1. Training Set: The model uses this subset to adjust its parameters (weights and biases) during training. It usually represents the largest part of the data, around 60-70% of the total dataset.\n",
    "\n",
    "2. Validation Set: This subset is used to evaluate the model's performance during training and to adjust hyperparameters (such as the number of layers, number of neurons, etc.). The validation set does not directly influence the adjustment of the model's parameters but provides feedback to prevent overfitting and to optimize the model. It is usually around 10-20% of the dataset.\n",
    "\n",
    "3. Test Set: After the model is trained and optimized using the validation set, the test set is used to evaluate the final performance of the model. The purpose of this subset is to provide an estimate of the model's ability to generalize to new and unknown data. It is usually around 10-20% of the dataset.\n",
    "\n",
    "\n",
    "##### **3. Training the MLP**\n",
    "\n",
    "To find the optimal parameters (weights and biases) of the model during training, a loss function is defined that measures how well the network's predictions correspond to the actual labels.\n",
    "\n",
    "Thus, in the case of binary classification, the loss function used is **binary cross-entropy**:\n",
    "$$\n",
    "\\text{BCELoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "- where N represents the total number of examples\n",
    "- $y_i$ is the real label (0 or 1)\n",
    "- $\\hat y_i$ is the probability predicted by the model for the positive class (1).\n",
    "\n",
    "To minimize the loss function, an iterative optimization process such as **gradient descent** is usually used. This algorithm iteratively updates the parameters in the opposite direction of the gradient to ensure the loss function decreases.\n",
    "\n",
    "The parameter update is performed as follows:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} \\text{BCELoss}\n",
    "$$\n",
    "\n",
    "- where Î± is the learning rate\n",
    "- $\\nabla_{\\theta} \\text{BCELoss}$ is the gradient of the loss function with respect to the parameters and contains the partial derivatives of the loss function with respect to each parameter of the model\n",
    "\n",
    "Gradient descent can be applied in three main ways: **batch gradient descent**, **stochastic gradient descent (SGD)**, and **mini-batch gradient descent**.\n",
    "\n",
    "1. Batch gradient descent: The gradient is calculated using the entire training set at each update step.\n",
    "2. Stochastic gradient descent: The gradient is calculated and the parameters are updated for each example in the training set.\n",
    "3. Mini-Batch Gradient Descent: The dataset is divided into small subsets (mini-batches), and the gradient is calculated and the parameters are updated for each mini-batch.\n",
    "\n",
    "The steps for training an MLP are:\n",
    "\n",
    "1. **Forward Pass**: The input data is passed through the network layer by layer, from the input layer to the output layer. Each neuron calculates a linear combination of the inputs (the weighted sum of the inputs plus a bias term), applies an activation function, and passes the output on. This phase produces the network's prediction for a given set of inputs.\n",
    "\n",
    "2. **Error Calculation** (Loss Function): After the prediction is obtained, it is compared with the actual value (the true label).\n",
    "\n",
    "3. **Backpropagation**: In this phase, the partial derivatives of the loss function with respect to each parameter of the model are calculated by propagating the derivatives from the output layer to the hidden layers using the chain rule.\n",
    "\n",
    "6. **Weight Update**: The weights are adjusted in the descending direction of the gradient using the gradient descent method.\n",
    "\n",
    "##### **4. Testing the MLP**\n",
    "\n",
    "In the testing phase of an MLP, the goal is to evaluate its performance on new, unknown data that was not used in training or validation. In this stage, the model only performs the forward pass to make predictions, without modifying the parameters.\n",
    "\n",
    "In the case of binary classification, the obtained probability $p(y=1|x; \\theta)$ will be compared with a threshold to find the predicted class:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\text{if } p(y=1|x; \\theta) < 0.5 \\text{ predict class 0} \\\\\n",
    "\\text{if } p(y=1|x; \\theta) \\geq 0.5 \\text{ predict class 1}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In the exercises in this lab, you will use a neural network to recognize two handwritten digits, 0 and 1, from images in the MNIST dataset. This is a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7706f07-6c35-4e46-89cd-1e71457c6337",
   "metadata": {},
   "source": [
    "**Ex1. Reading and visualizing the data.**\n",
    "\n",
    "The training set contains 12665 examples of handwritten digits, zero and one, stored in the `X_train.npy` file.\n",
    "\n",
    "Each training example is a 28 x 28 pixel grayscale image of the respective digit. The 28 x 28 pixel matrix is flattened into a vector of size 784. Thus, each training example becomes a row in the feature matrix X. We have an X matrix of size 12665 x 784, where each row is a training example of a handwritten digit image.\n",
    "$$\n",
    "X = \n",
    "\\left(\\begin{array}{cc} \n",
    "--- (x^{(1)}) --- \\\\\n",
    "--- (x^{(2)}) --- \\\\\n",
    "\\vdots \\\\ \n",
    "--- (x^{(m)}) --- \n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The `Y_train.npy` file stores the corresponding labels for each training example:\n",
    "- if y = 0 the image represents the digit 0\n",
    "- if y = 1 the image represents the digit 1\n",
    "\n",
    "Also, the `X_test.npy` and `Y_test.npy` files contain the test data in the same format.\n",
    "\n",
    "The training and test data will be read and the images will be visualized as follows:\n",
    "\n",
    "- 5 random images are selected, the 28x28 images are formed and displayed in a grid\n",
    "- the label of each image will be displayed above the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "753d5eca-dec1-4cbf-b88c-8b005397972f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:18:19.295658Z",
     "start_time": "2025-12-19T07:18:18.860332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = np.load('MNIST/X_train.npy')\n",
    "X_test = np.load('MNIST/X_test.npy')\n",
    "y_train = np.load('MNIST/Y_train.npy')\n",
    "y_test = np.load('MNIST/Y_test.npy')\n",
    "\n",
    "\n",
    "fig_train, ax_train = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
    "fig_train.suptitle(\"TRAIN set\", y=0.73, fontsize=16, fontweight='bold')\n",
    "\n",
    "fig_test, ax_test = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
    "fig_test.suptitle(\"TEST set\", y=0.73, fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx in range(5):\n",
    "    i, j = np.random.randint(X_train.shape[0]), np.random.randint(X_test.shape[0])\n",
    "    \n",
    "    ax_train[idx].imshow(X_train[i].reshape(28, 28), cmap = matplotlib.cm.binary)\n",
    "    ax_train[idx].set_title(str(y_train[i]))\n",
    "    ax_train[idx].axis('off')\n",
    "    \n",
    "    ax_test[idx].imshow(X_test[j].reshape(28, 28), cmap = matplotlib.cm.binary)\n",
    "    ax_test[idx].set_title(str(y_test[j]))\n",
    "    ax_test[idx].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2dbbe-7c1d-4529-9699-7b68bea43ae1",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "The neural network has three dense layers with ReLU activation functions in the hidden layers and the sigmoid activation function in the last layer.\n",
    "\n",
    "- The inputs are the pixel values from the digit images. Since the images have a size of 28 x 28 pixels, this results in 784 inputs.\n",
    "- The neural network has 25 neurons in layer 1, 15 neurons in layer 2, and 1 neuron in layer 3.\n",
    "\n",
    "The dimensions of the parameters of the dense layer network are determined as follows:\n",
    "\n",
    "If the network has $n$ neurons in one layer and $m$ neurons in the next layer, then the weights $W$ will have the dimension $(n, m)$, and the bias $b$ will be a vector with $m$ elements.\n",
    "\n",
    "Thus, the dimensions of the matrices $W$ and vectors $b$ are:\n",
    "\n",
    "- **Layer 1**: The dimension of $W_1$ is (784, 25), and the dimension of $b_1$ is (1, 25).\n",
    "- **Layer 2**: The dimension of $W_2$ is (25, 15), and the dimension of $b_2$ is (1, 15).\n",
    "- **Layer 3**: The dimension of $W_3$ is (15, 1), and the dimension of $b_3$ is (1, 1).\n",
    "\n",
    "For a training example, each neuron calculates the dot product between the input vector and a column from the weight vector, after which it applies the activation function.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/mlp_architecture.png\" alt=\"image\" width=\"400\" title=\"Neuron\"/>\n",
    "</p> \n",
    "<p align=\"center\"><em>Figure 4. Neural network. </em></p>\n",
    "\n",
    "**Parameter Initialization**\n",
    "\n",
    "1. The bias (b) is usually initialized with 0 or a small constant value, for example 0.01.\n",
    "\n",
    "\n",
    "2. The weights can be initialized with random numbers drawn from a Gaussian distribution.\n",
    "\n",
    "$$\n",
    "w \\sim \\mathcal{N}\\left(0, 1 \\right)\n",
    "$$\n",
    "\n",
    "This assumes that the neurons in all layers will work well with values drawn from the same distribution, regardless of their size. In practice, this can cause problems for the following reasons:\n",
    "\n",
    "- Weights too small: The gradients shrink as they propagate through the layers, eventually becoming so small that the network stops learning. This phenomenon is known as the vanishing gradients problem.\n",
    "\n",
    "- Weights too large: The gradients grow uncontrollably as they propagate through the layers, leading to the exploding gradients problem.\n",
    "\n",
    "A solution to this problem is He initialization [1], which is used especially in the context of using the ReLU activation function. Thus, the weights are drawn from a normal distribution with the variance scaled by the number of inputs to the neuron $n_{in}$.\n",
    "\n",
    "$$\n",
    "w' \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)\n",
    "$$\n",
    "\n",
    "In practice, the implementation involves multiplying the values drawn from the Gaussian distribution $\\mathcal{N}\\left(0, 1 \\right)$ by the scaling factor as follows:\n",
    "\n",
    "$$\n",
    "w' = w \\times \\sqrt{\\frac{2}{n_{in}}}\n",
    "$$\n",
    "\n",
    "[1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015.\n",
    "\n",
    "**Ex1. Implement a 3-layer neural network.** Implement the forward propagation algorithm for the model presented in Figure 4 and use He initialization for the weights $(W_1, W_2, W_3)$ and zero initialization for the biases $(b_1, b_2, b_3)$. For a single training example, run the forward propagation and display the dimensions of the output vectors, weights, and biases after each layer.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 &= X \\cdot W_1 + b_1, \\quad a_1 = \\text{ReLU}(z_1) \\\\\n",
    "z_2 &= a_1 \\cdot W_2 + b_2, \\quad a_2 = \\text{ReLU}(z_2) \\\\\n",
    "z_3 &= a_2 \\cdot W_3 + b_3, \\quad a_3 = \\text{Sigmoid}(z_3)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> Make a general implementation that can also be used for an X with more training examples. <br>\n",
    "> Vectorize the multiplication operations. <br>\n",
    "> To generate values from a Gaussian distribution, see the `np.random` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fbc2199-b952-4fcd-8ac1-724761145255",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:18:19.311336Z",
     "start_time": "2025-12-19T07:18:19.306229Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size1 = 25\n",
    "hidden_size2 = 15\n",
    "output_size = 1\n",
    "\n",
    "W1 = np.random.normal(loc=0, scale=np.sqrt(2.0 / input_size), size=(input_size, hidden_size1))\n",
    "W2 = np.random.normal(loc=0, scale=np.sqrt(2.0 / hidden_size1), size=(hidden_size1, hidden_size2))\n",
    "W3 = np.random.normal(loc=0, scale=np.sqrt(2.0 / hidden_size2), size=(hidden_size2, 1))\n",
    "\n",
    "b1 = np.zeros(hidden_size1)\n",
    "b2 = np.zeros(hidden_size2)\n",
    "b3 = np.zeros(1)\n",
    "\n",
    "params = {\n",
    "    \"W1\": W1, \"b1\": b1,\n",
    "    \"W2\": W2, \"b2\": b2,\n",
    "    \"W3\": W3, \"b3\": b3\n",
    "}\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Input array or scalar (can be a numpy array, list, or single numeric value).\n",
    "\n",
    "    Returns:\n",
    "    - result: The sigmoid of the input, element-wise if `x` is an array (same shape as `x`).\n",
    "    \"\"\"\n",
    "    # return (lambda val: 1 / (1 + np.exp(-val)))(x)\n",
    "    # return np.array([1 / (1 + np.exp(-val)) for val in x])\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Computes the ReLU (Rectified Linear Unit) activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Input array or scalar (can be a numpy array, list, or single numeric value).\n",
    "\n",
    "    Returns:\n",
    "    - result: The ReLU of the input, element-wise if `x` is an array (same shape as `x`).\n",
    "    \"\"\"\n",
    "    # return (lambda val: max(0, val))(x)\n",
    "    # return np.array([max(0, val) for val in x])\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def forward(X, params):\n",
    "    \"\"\"\n",
    "    Performs the forward pass of the neural network.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data (shape: (m, input_size)).\n",
    "    - params: Dictionary containing weights and biases.\n",
    "\n",
    "    Returns:\n",
    "    - activations: Dictionary containing intermediate and final outputs.\n",
    "    \"\"\"\n",
    "    W1, b1 = params[\"W1\"], params[\"b1\"]\n",
    "    W2, b2 = params[\"W2\"], params[\"b2\"]\n",
    "    W3, b3 = params[\"W3\"], params[\"b3\"]\n",
    "\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = relu(z2)\n",
    "\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    activations = {\n",
    "        \"z1\": z1, \"a1\": a1,\n",
    "        \"z2\": z2, \"a2\": a2,\n",
    "        \"z3\": z3, \"a3\": a3\n",
    "    }\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b60fc-9a20-49c2-8acb-378b5c964d5b",
   "metadata": {},
   "source": [
    "**Ex2. Implement the binary cross-entropy loss function.**\n",
    "\n",
    "$$\n",
    "\\text{BCELoss} = -\\frac{1}{M} \\sum_{i=1}^{M} \\left( y_i \\times \\log(\\hat{y}_i) + (1 - y_i) \\times \\log(1 - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "- where M represents the number of examples\n",
    "- $y_i$ is the real label (0 or 1)\n",
    "- $\\hat y_i$ is the probability predicted by the model for the positive class (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75168065-a68c-4f8a-bde6-bb93bbab285a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:18:19.360508Z",
     "start_time": "2025-12-19T07:18:19.357960Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy loss for a batch of predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels (shape: (m, 1) or (m,), where m is the batch size). \n",
    "              Values should be binary (0 or 1).\n",
    "    - y_pred: Predicted probabilities (shape: (m, 1) or (m,)). \n",
    "              Values should be in the range [0, 1], representing the model's confidence (probability)\n",
    "              in predicting the positive class.\n",
    "\n",
    "    Returns:\n",
    "    - loss: The average binary cross-entropy loss for the batch (scalar value).\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    M = y_true.shape[0]\n",
    "    \n",
    "    loss = -1 / M * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f5e2b-cb85-4d9c-8022-8ec1bc0c71c1",
   "metadata": {},
   "source": [
    "**Ex3. Calculate the gradient of the BCELoss loss function with respect to $\\hat{y}$.** The gradient $\\nabla_\\hat{y} \\text{BCELoss}$ will be a vector containing the partial derivatives of the loss function with respect to each $\\hat{y}_i$, namely $[\\frac{\\partial \\text{BCELoss}}{\\partial \\hat{y}_0}, \\frac{\\partial \\text{BCELoss}}{\\partial \\hat{y}_1}, ...]$ \n",
    "Both y and $\\hat{y}$ are vectors. The gradient $\\nabla_\\hat{y} \\text{BCELoss}$ has the same dimension as $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c525e53-0c18-4c84-bc8e-eb72c71cf3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:18:19.410396Z",
     "start_time": "2025-12-19T07:18:19.407817Z"
    }
   },
   "outputs": [],
   "source": [
    "def BCE_loss_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the binary cross-entropy (BCE) loss \n",
    "    with respect to the predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels (shape: (m, 1) or (m,), where m is the batch size).\n",
    "              Values should be binary (0 or 1).\n",
    "    - y_pred: Predicted probabilities (shape: (m, 1) or (m,)).\n",
    "              Values should be in the range [0, 1], representing the model's confidence\n",
    "              in predicting the positive class.\n",
    "\n",
    "    Returns:\n",
    "    - gradient: The derivative of the binary cross-entropy loss with respect to \n",
    "                the predictions (y_pred). This has the same shape as `y_pred`.\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    gradient = -(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "    return gradient / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bacdea5f-61b1-4ce1-b923-acffa55db92c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:18:19.463817Z",
     "start_time": "2025-12-19T07:18:19.459117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case: 1\n",
      "Test: PASS\n",
      "Test case: 2\n",
      "Test: PASS\n"
     ]
    }
   ],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "test_cases = [\n",
    "    {\"y_hat\": np.array([0.8, 0.1, 0.5]), \"y\": np.array([1, 0, 1]), \"expected_derivative\": np.array([-0.41666667, 0.37037037, -0.66666667])},\n",
    "    {\"y_hat\": np.array([[0.4], [0.8], [0.2]]), \"y\": np.array([[0], [1], [0]]), \"expected_derivative\": np.array([[0.55555556],[-0.41666667],[ 0.41666667]])}]\n",
    "\n",
    "for i, case in enumerate(test_cases, start=1):\n",
    "    y_hat = case[\"y_hat\"]\n",
    "    y = case[\"y\"]\n",
    "    expected_derivative = case[\"expected_derivative\"]\n",
    "    \n",
    "    computed_derivative = BCE_loss_derivative(y, y_hat)\n",
    "\n",
    "    matched = np.allclose(computed_derivative, expected_derivative, atol=1e-4)\n",
    "    \n",
    "    print(f\"Test case: {i}\")\n",
    "    print(\"Test:\", \"PASS\" if matched else \"FAIL\")\n",
    "    \n",
    "    if not matched:\n",
    "        print(f\"Expected Derivative: {expected_derivative}\")\n",
    "        print(f\"Computed Derivative: {computed_derivative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c19d2b-06f9-4b87-b130-f133c66995a8",
   "metadata": {},
   "source": [
    "**Ex4. Calculate the derivative of the sigmoid function $\\sigma(x)$ with respect to an input x.**\n",
    "\n",
    "$$ \\sigma'(x) = \\sigma(x) \\times (1 - \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a4639d3-b43a-40a5-a2c7-701410e614be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:18:19.518055Z",
     "start_time": "2025-12-19T07:18:19.515692Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Input array or scalar (can be a numpy array, list, or single numeric value).\n",
    "         Represents the input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - result: The derivative of the sigmoid function, element-wise if `x` is an array.\n",
    "              It has the same shape as `x`.\n",
    "    \"\"\"\n",
    "    # return np.dot(sigmoid(x), 1 - sigmoid(x))\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5057de-883b-4c18-98d6-99d26f048fb5",
   "metadata": {},
   "source": [
    "Next, we will calculate the gradient of the loss function with respect to each parameter of the model using the backpropagation method.\n",
    "\n",
    "**Simple example of backpropagation.**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/computational_graph.png\" alt=\"image\" width=\"800\"/>\n",
    "</p> \n",
    "\n",
    "\n",
    "For the MLP model, we will start by calculating the derivative of the loss function with respect to $a_3$ (the predicted probability also denoted by $\\hat{y}$), after which we will use the chain rule and backpropagation to calculate all the derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{i,k}} = \\frac{\\partial L}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial w_{i,k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba05b5-a4c7-4e27-a54c-bc9b1a5310b5",
   "metadata": {},
   "source": [
    "**Ex5. Calculate the gradient of the BCE Loss function for the last layer of the model** for a batch of X examples of size (m, input_size). We will calculate $\\frac{\\partial L}{\\partial W_3}$ and $\\frac{\\partial L}{\\partial b_3}$.\n",
    "\n",
    "\n",
    "In backpropagation, we encounter 2 types of multiplications:\n",
    "- if in forward propagation we have a **linear transformation**, i.e., a matrix multiplication or a dot product $z = x \\cdot W + b$, then in backpropagation we also use a dot product or matrix multiplication to propagate the gradients.\n",
    "\n",
    "The gradient of W is calculated with the chain rule as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{L}}{\\partial W} = x^T \\cdot \\frac{\\partial \\text{L}}{\\partial z}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial \\text{L}}{\\partial z}$ represents the gradient coming from the previous layer, i.e., from \"upstream\", and the gradient of x is calculated as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{L}}{\\partial x} = \\frac{\\partial \\text{L}}{\\partial z} \\cdot W^T \n",
    "$$\n",
    "\n",
    "- if in forward propagation we have an operation that is applied **element-wise** to a matrix or a vector (such as ReLU, Sigmoid, multiplication by a scalar), then we will use element-wise multiplication in backpropagation.\n",
    "\n",
    "\n",
    "For the last layer that applies a linear transformation $z_3 = a_2 \\cdot W_3 + b_3$, where the dimensions are $a_2(m,p)$, $W_3(p,1)$, $b_3(1, 1)$, $z_3(m, 1)$, $a_3(m, 1)$, and m is the number of examples in the batch, the chain rule is applied as follows:\n",
    "\n",
    "**Forward propagation $\\rightarrow$**:\n",
    "\n",
    "$$\n",
    "z_3 = a_2 \\cdot W_3 + b_3\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_3 = \\text{Sigmoid}(z_3)\n",
    "$$\n",
    "\n",
    "**Backpropagation $\\leftarrow$**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial z_3} = \\frac{\\partial \\text{BCELoss}}{\\partial a_3} \\times \\frac{\\partial a_3}{\\partial z_3} \\text{(element-wise multiplication)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial W_3} = \\frac{1}{m}a_2^T \\cdot \\frac{\\partial \\text{BCELoss}}{\\partial z_3}  \\text{(matrix multiplication)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial b_3} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\frac{\\partial  \\text{BCELoss}}{\\partial z_3^i} \\text{(mean over columns)}\n",
    "$$ \n",
    "\n",
    "> *Hint*: Call the propagation function for the batch of input examples only once. <br>\n",
    "> The dimension of the gradient with respect to a parameter is always equal to the dimension of the parameter <br>\n",
    "> $a_3$ actually represents $\\hat{y}$ used in the `BCELoss` calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e9306f3-e202-4d08-9bea-6253433741ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:18:19.568971Z",
     "start_time": "2025-12-19T07:18:19.564852Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "def backward_third_layer(y: np.ndarray, activations: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes vectorized gradients of the last layer for backpropagation over a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - activations (Dict[str, np.ndarray]): Dictionary containing activations and pre-activations of the network.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        Gradients computed for the last layer:\n",
    "        - dz3 : Gradient of the loss w.r.t. the pre-activation (z3), shape (m, 1).\n",
    "        - dW3  : Gradient of the loss w.r.t. the weights (W3), shape (p, 1).\n",
    "        - db3  : Gradient of the loss w.r.t. the biases (b3), shape (1, 1).\n",
    "\n",
    "    \"\"\"\n",
    "    a3 = activations[\"a3\"]\n",
    "    z3 = activations[\"z3\"]\n",
    "    a2 = activations[\"a2\"]\n",
    "\n",
    "    m = y.shape[0]\n",
    "\n",
    "    dL_da3 = BCE_loss_derivative(y, a3)\n",
    "    da3_dz3 = sigmoid_derivative(z3)\n",
    "    \n",
    "    dz3 = dL_da3 * da3_dz3\n",
    "\n",
    "    dW3 = (1 / m) * np.dot(a2.T, dz3)\n",
    "\n",
    "    db3 = (1 / m) * np.sum(dz3, axis=0, keepdims=True)\n",
    "\n",
    "    return dz3, dW3, db3\n",
    "\n",
    "\n",
    "def forward_backward_third_layer(X: np.ndarray, \n",
    "                                y: np.ndarray, \n",
    "                                params: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "     Performs a full forward pass for the entire network to compute the intermediate activations \n",
    "    and backward pass for the last layer for a single example.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, p).\n",
    "                      `m` is the number of examples, `p` is the number of features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W3, b3).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray]: Gradients (dz3, dW3, db3)\n",
    "    \"\"\"\n",
    "    activations = forward(X, params)\n",
    "    \n",
    "    dz3, dW3, db3 = backward_third_layer(y, activations)\n",
    "    \n",
    "    return dz3, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e5dd31f-7660-4791-af47-a0f53ba7749d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:18:19.625331Z",
     "start_time": "2025-12-19T07:18:19.620542Z"
    }
   },
   "outputs": [],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "def test_backward(forward_backward_func, params, X_batch, y_batch, expected_dW, expected_db):\n",
    "     \n",
    "    # Run forward and backward pass\n",
    "    W1, b1 = params[\"W1\"], params[\"b1\"]\n",
    "    W2, b2 = params[\"W2\"], params[\"b2\"]\n",
    "    W3, b3 = params[\"W3\"], params[\"b3\"]\n",
    "    _, dW, db = forward_backward_func(X_batch, y_batch, params)\n",
    "      \n",
    "    dW_pass = np.allclose(dW, expected_dW, atol=1e-4)\n",
    "    db_pass = np.allclose(db, expected_db, atol=1e-4)\n",
    "    \n",
    "    print(\"Test Results:\")\n",
    "    print(\"dW Test:\", \"PASS\" if dW_pass else \"FAIL\")\n",
    "    print(\"db Test:\", \"PASS\" if db_pass else \"FAIL\")\n",
    "    \n",
    "    if not dW_pass:\n",
    "        print(\"\\nMismatch in dW:\")\n",
    "        print(\"Computed dW:\")\n",
    "        print(dW)\n",
    "        print(\"Expected dW:\")\n",
    "        print(expected_dW)\n",
    "    \n",
    "    if not db_pass:\n",
    "        print(\"\\nMismatch in db:\")\n",
    "        print(\"Computed db:\")\n",
    "        print(db)\n",
    "        print(\"Expected db:\")\n",
    "        print(expected_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83a3eeb6-8531-4bfb-b38a-c66317f94df7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:18:19.685158Z",
     "start_time": "2025-12-19T07:18:19.676879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "dW Test: PASS\n",
      "db Test: PASS\n"
     ]
    }
   ],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "W1 = np.array([[0.1, -0.05, 0.02],\n",
    "               [0.03, 0.07, -0.02]])\n",
    "b1 = np.array([[0.01, 0.02, -0.03]])\n",
    "W2 = np.array([[0.1, -0.1],\n",
    "               [0.05, 0.02],\n",
    "               [-0.03, 0.06]])\n",
    "b2 = np.array([[0.03, -0.02]])\n",
    "\n",
    "W3 = np.array([[5.0], [-3.0]])  \n",
    "b3 = np.array([[0.5]])          \n",
    "\n",
    "params = {\n",
    "    \"W1\": W1, \"b1\": b1,\n",
    "    \"W2\": W2, \"b2\": b2,\n",
    "    \"W3\": W3, \"b3\": b3\n",
    "}\n",
    "\n",
    "X_t = np.array([[1.5, -0.3], [0.7, 2.2]])  \n",
    "y_t = np.array([[1], [0]]) \n",
    "\n",
    "expected_dW3 = np.array([[0.00510939],[0.]])\n",
    "expected_db3 = np.array([[0.08862105]])\n",
    "\n",
    "test_backward(forward_backward_third_layer, params, X_t, y_t, expected_dW3, expected_db3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95daccf2-c383-4d5a-b6ce-c4115c7c8fe3",
   "metadata": {},
   "source": [
    "**Ex6. Calculate the gradient of the loss function for the second layer of the model for a batch of examples.** To calculate $\\frac{\\partial L}{\\partial W_2}$ and $\\frac{\\partial L}{\\partial b_2}$, use the gradients calculated so far and backpropagation.\n",
    "\n",
    "**Forward propagation $\\rightarrow$:**\n",
    "$$\n",
    "z_2 = a_1 \\cdot W_2 + b_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_2 = \\text{ReLU}(z_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_3 = a_2 \\cdot W_3 + b_3\n",
    "$$\n",
    "\n",
    "**Backpropagation $\\leftarrow$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial a_2} = \\frac{\\partial \\text{BCELoss}}{\\partial z_3} \\cdot W_3^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial z_2} = \\frac{\\partial \\text{BCELoss}}{\\partial a_2} \\times ReLU'(z_2)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\text{ReLU'}(x) =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } x > 0 \\\\\n",
    "0, & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial W_2} = \\frac{1}{m}a_1^T \\cdot \\frac{\\partial \\text{BCELoss}}{\\partial z_2}  \\text{(matrix multiplication)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial b_2} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\frac{\\partial \\text{BCELoss}}{\\partial z_2^i} \\text{(mean over columns)}\n",
    "$$ \n",
    "\n",
    "> *Hint*: Reuse the `backward_third_layer` function to calculate $\\frac{\\text{BCELoss}}{\\partial z_3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e401fb9-65be-4186-9d65-03648fe11734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:19:26.462544Z",
     "start_time": "2025-12-19T07:19:26.458622Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def backward_second_layer(y: np.ndarray, \n",
    "                          params: Dict[str, np.ndarray], \n",
    "                          activations: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes vectorized gradients of the second layer for backpropagation over a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W2, b2, W3, b3).\n",
    "    - activations (Dict[str, np.ndarray]): Dictionary containing activations and pre-activations of the network.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        Gradients computed for the last layer:\n",
    "        - dz2 : Gradient of the loss w.r.t. the pre-activation (z2)\n",
    "        - dW2  : Gradient of the loss w.r.t. the weights (W2)\n",
    "        - db2  : Gradient of the loss w.r.t. the biases (b2)\n",
    "\n",
    "    \"\"\"\n",
    "    W3 = params[\"W3\"]\n",
    "    z2 = activations[\"z2\"]\n",
    "    a1 = activations[\"a1\"]\n",
    "    m = y.shape[0]\n",
    "\n",
    "    a3 = activations[\"a3\"]\n",
    "    z3 = activations[\"z3\"]\n",
    "    dL_da3 = BCE_loss_derivative(y, a3)\n",
    "    da3_dz3 = sigmoid_derivative(z3)\n",
    "    dz3 = dL_da3 * da3_dz3\n",
    "\n",
    "    dz2 = np.dot(dz3, W3.T) * relu_derivative(z2)\n",
    "    dW2 = (1 / m) * np.dot(a1.T, dz2)\n",
    "    db2 = (1 / m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    return dz2, dW2, db2\n",
    "\n",
    "def forward_backward_second_layer(X: np.ndarray, \n",
    "                                  y: np.ndarray, \n",
    "                                  params: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs a full forward pass for the entire network to compute the intermediate activations \n",
    "    and backward pass for the second layer using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, n).\n",
    "                      `m` is the number of examples, `n` is the number of features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W2, b2, W3, b3).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray]: Gradients (dz2, dW2, db2)\n",
    "    \"\"\"\n",
    "    activations = forward(X, params)\n",
    "    dz2, dW2, db2 = backward_second_layer(y, params, activations)\n",
    "    return dz2, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad41616a-d6cb-47b5-a0a4-e5be67a3cb82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:19:29.101609Z",
     "start_time": "2025-12-19T07:19:29.096606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "dW Test: PASS\n",
      "db Test: PASS\n"
     ]
    }
   ],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "W1 = np.array([[0.1, -0.05, 0.02],\n",
    "               [0.03, 0.07, -0.02]])\n",
    "b1 = np.array([[0.01, 0.02, -0.03]])\n",
    "W2 = np.array([[0.1, 0.1],\n",
    "               [4.0, 2.0],\n",
    "               [-0.03, 0.06]])\n",
    "b2 = np.array([[0.03, -0.02]])\n",
    "\n",
    "W3 = np.array([[5.0], [-3.0]])  \n",
    "b3 = np.array([[0.5]])          \n",
    "\n",
    "params = {\n",
    "    \"W1\": W1, \"b1\": b1,\n",
    "    \"W2\": W2, \"b2\": b2,\n",
    "    \"W3\": W3, \"b3\": b3\n",
    "}\n",
    "\n",
    "X_t = np.array([[1.5, -0.3], [0.7, 2.2]])  \n",
    "y_t = np.array([[1], [0]]) \n",
    "\n",
    "expected_dW2 = np.array([[ 0.10924257, -0.10250817],[ 0.16265566, -0.09759339], [-0.00244786, 0]])\n",
    "expected_db2 = np.array([[0.76220853, -0.70211075]])\n",
    "\n",
    "test_backward(forward_backward_second_layer, params, X_t, y_t, expected_dW2, expected_db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937fcba7-b71e-40d7-a164-942eb3b9509f",
   "metadata": {},
   "source": [
    "**Ex7. Calculate the gradient of the loss function for the first layer of the model for a batch of examples.** To calculate $\\frac{\\partial L}{\\partial W_1}$ and $\\frac{\\partial L}{\\partial b_1}$, use the gradients calculated so far and backpropagation.\n",
    "\n",
    "\n",
    "**Forward propagation $\\rightarrow$**\n",
    "$$\n",
    "z_1 = X \\cdot W_1 + b_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_1 = \\text{ReLU}(z_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_2 = a_1 \\cdot W_2 + b_2\n",
    "$$\n",
    "\n",
    "\n",
    "**Backpropagation $\\leftarrow$**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial a_1} = \\frac{\\partial \\text{BCELoss}}{\\partial z_2} \\cdot W_2^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial z_1} = \\frac{\\partial \\text{BCELoss}}{\\partial a_1} \\times ReLU'(z_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial W_1} = \\frac{1}{m}X^T \\cdot \\frac{\\partial \\text{BCELoss}}{\\partial z_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCELoss}}{\\partial b_1} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\frac{\\partial \\text{BCELoss}}{\\partial z_1^i}\n",
    "$$ \n",
    "\n",
    "> *Hint*: Reuse the `backward_second_layer` function to calculate $\\frac{\\text{BCELoss}}{\\partial z_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0255963-970d-4527-9dc9-51c1ec013216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_first_layer(X: np.ndarray, \n",
    "                                          y: np.ndarray, \n",
    "                                          params: Dict[str, np.ndarray], \n",
    "                                          activations: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes vectorized gradients of the first layer for backpropagation over a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, n).\n",
    "                      `m` is the number of examples, `n` is the number of input features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W1, b1, W2, b2, W3, b3).\n",
    "    - activations (Dict[str, np.ndarray]): Dictionary containing activations and pre-activations of the network.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        Gradients computed for the last layer:\n",
    "        - dz1 : Gradient of the loss w.r.t. the pre-activation (z1)\n",
    "        - dW1  : Gradient of the loss w.r.t. the weights (W1)\n",
    "        - db1  : Gradient of the loss w.r.t. the biases (b1)\n",
    "    \"\"\"\n",
    "    W2 = params[\"W2\"]\n",
    "    W3 = params[\"W3\"]\n",
    "    z1 = activations[\"z1\"]\n",
    "    z2 = activations[\"z2\"]\n",
    "    z3 = activations[\"z3\"]\n",
    "    a3 = activations[\"a3\"]\n",
    "\n",
    "    m = X.shape[0]\n",
    "\n",
    "    dz3 = BCE_loss_derivative(y, a3) * sigmoid_derivative(z3)\n",
    "    dz2 = np.dot(dz3, W3.T) * relu_derivative(z2)\n",
    "    dz1 = np.dot(dz2, W2.T) * relu_derivative(z1)\n",
    "\n",
    "    dW1 = (1 / m) * np.dot(X.T, dz1)\n",
    "    db1 = (1 / m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    return dz1, dW1, db1\n",
    "\n",
    "def forward_backward_first_layer(X: np.ndarray, \n",
    "                        y: np.ndarray, \n",
    "                        params: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs a full forward pass for the entire network to compute the intermediate activations \n",
    "    and backward pass for the first layer using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, n).\n",
    "                      `m` is the number of examples, `n` is the number of input features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W1, b1, W2, b2, W3, b3).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray]: Gradients (dW1, db1) for the first layer weights and biases.\n",
    "    \"\"\"\n",
    "    activations = forward(X, params)\n",
    "\n",
    "    dz1, dW1, db1 = backward_first_layer(X, y, params, activations)\n",
    "    \n",
    "    return dz1, dW1, db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c69d9bd4-6ed2-4aea-8aec-677b2751818f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "dW Test: PASS\n",
      "db Test: PASS\n"
     ]
    }
   ],
   "source": [
    "### RUN TESTS\n",
    "\n",
    "W1 = np.array([[0.1, -0.05, 0.2],\n",
    "               [0.3, 0.7, -0.02]])\n",
    "b1 = np.array([[0.1, 0.2, -0.3]])\n",
    "W2 = np.array([[0.1, 0.1],\n",
    "               [4.0, 2.0],\n",
    "               [-0.03, 0.06]])\n",
    "b2 = np.array([[0.03, -0.02]])\n",
    "\n",
    "W3 = np.array([[5.0], [-3.0]])  \n",
    "b3 = np.array([[0.5]])          \n",
    "\n",
    "params = {\n",
    "    \"W1\": W1, \"b1\": b1,\n",
    "    \"W2\": W2, \"b2\": b2,\n",
    "    \"W3\": W3, \"b3\": b3\n",
    "}\n",
    "\n",
    "X_t = np.array([[1.5, -0.3], [0.7, 2.2]])  \n",
    "y_t = np.array([[1], [0]]) \n",
    "\n",
    "expected_dW1 = np.array([[-2.60110482e-02, 2.45, 1.83033145e-02],\n",
    " [ 1.22202210e-01, 7.7, -3.66066289e-03]])\n",
    "expected_db1 = np.array([[ 0.00932597, 3.5, 0.01220221]])\n",
    "\n",
    "test_backward(forward_backward_first_layer, params, X_t, y_t, expected_dW1, expected_db1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afcb48-d08b-4633-a4ce-59b0964eabb6",
   "metadata": {},
   "source": [
    "**Ex8. Implement the `backward` function that calculates the gradients of the loss function with respect to all model parameters ($W_1, W_2, W_3, b_1, b_2, b_3$) and updates each parameter individually with gradient descent**:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} \\text{BCELoss}\n",
    "$$\n",
    "\n",
    "- where $\\alpha$ is the learning rate\n",
    "- $\\theta$ represents each parameter of the model ($W_1, W_2, W_3, b_1, b_2, b_3$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4967bf3c-4eb8-4bf9-af20-7058875a4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward(X: np.ndarray, \n",
    "             y: np.ndarray, \n",
    "             params: Dict[str, np.ndarray], \n",
    "             activations: Dict[str, np.ndarray], \n",
    "             learning_rate: float = 0.001) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Performs the backward pass of the neural network and updates the parameters using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Input features for the batch, shape (m, n).\n",
    "                      `m` is the number of examples, `n` is the number of input features.\n",
    "    - y (np.ndarray): True labels for the batch, shape (m, 1).\n",
    "                      `m` is the number of examples\n",
    "    - params (Dict[str, np.ndarray]): Dictionary containing model parameters (e.g., W1, b1, W2, b2, W3, b3).\n",
    "    - activations (Dict[str, np.ndarray]): Dictionary containing activations and pre-activations of the network.\n",
    "    - learning_rate (float, optional): Learning rate for gradient descent. Default is 0.001.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, np.ndarray]: Updated model parameters after gradient descent.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "\n",
    "    dz3, dW3, db3 = backward_third_layer(y, activations)\n",
    "\n",
    "    dz2 = np.dot(dz3, params[\"W3\"].T) * relu_derivative(activations[\"z2\"])\n",
    "    dW2 = (1 / m) * np.dot(activations[\"a1\"].T, dz2)\n",
    "    db2 = (1 / m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    dz1 = np.dot(dz2, params[\"W2\"].T) * relu_derivative(activations[\"z1\"])\n",
    "    dW1 = (1 / m) * np.dot(X.T, dz1)\n",
    "    db1 = (1 / m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    params[\"W3\"] -= learning_rate * dW3\n",
    "    params[\"b3\"] -= learning_rate * db3.flatten()\n",
    "\n",
    "    params[\"W2\"] -= learning_rate * dW2\n",
    "    params[\"b2\"] -= learning_rate * db2.flatten()\n",
    "\n",
    "    params[\"W1\"] -= learning_rate * dW1\n",
    "    params[\"b1\"] -= learning_rate * db1.flatten()\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4a77c-4c0f-4b09-8455-2956427a4d45",
   "metadata": {},
   "source": [
    "**Ex9. Implement the training function.** The $X_{train}$ dataset will be iterated for a fixed number of epochs. In one epoch, all examples from the training set are visited. Mini-batch gradient descent will be used, where only one batch of examples from the entire training set will be propagated forward at a time. Display the value of the loss function at each epoch. Display the graph of the loss function.\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "**Input**: \n",
    "- `X_train` (training data), `y_train` (training labels)\n",
    "\n",
    "**Hyperparameters**:\n",
    "- `num_epochs` (number of epochs)\n",
    "-  `batch_size` (mini-batch size)\n",
    "- `learning_rate` (learning rate)\n",
    "\n",
    "**Output**: \n",
    "- Prints average loss per epoch\n",
    "\n",
    "**Initialize Parameters**:\n",
    "   ```plaintext\n",
    "   num_epochs â 30\n",
    "   batch_size â 64  # Mini-batch size\n",
    "   learning_rate â 0.1\n",
    "   num_batches â floor(X_train.shape[0] / batch_size)  # Number of batches per epoch\n",
    "```\n",
    "\n",
    "**Training Loop**:\n",
    "```plaintext\n",
    "FOR epoch IN range(1, num_epochs + 1):\n",
    "    # Shuffle the data at the start of each epoch\n",
    "    indices â generate shuffled indices for X_train\n",
    "    X_train â reorder X_train using indices\n",
    "    y_train â reorder y_train using indices\n",
    "\n",
    "    epoch_loss â 0  # Initialize epoch loss\n",
    "\n",
    "    # Loop over mini-batches\n",
    "    FOR i IN range(num_batches):\n",
    "        start â i * batch_size\n",
    "        end â start + batch_size\n",
    "        \n",
    "        # Get the mini-batch\n",
    "        X_batch â X_train[start:end]\n",
    "        y_batch â y_train[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        output â forward(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss â binary_cross_entropy(y_batch, output)\n",
    "        epoch_loss â epoch_loss + loss\n",
    "        \n",
    "        # Backward pass\n",
    "        backward(X_batch, y_batch, output, learning_rate)\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    avg_epoch_loss â epoch_loss / num_batches\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0632bb5-3f46-4aae-8731-760a0deabcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 0.5425\n",
      "Epoch 2/30 - Loss: 0.3692\n",
      "Epoch 3/30 - Loss: 0.2496\n",
      "Epoch 4/30 - Loss: 0.1624\n",
      "Epoch 5/30 - Loss: 0.1085\n",
      "Epoch 6/30 - Loss: 0.0777\n",
      "Epoch 7/30 - Loss: 0.0594\n",
      "Epoch 8/30 - Loss: 0.0476\n",
      "Epoch 9/30 - Loss: 0.0400\n",
      "Epoch 10/30 - Loss: 0.0344\n",
      "Epoch 11/30 - Loss: 0.0303\n",
      "Epoch 12/30 - Loss: 0.0271\n",
      "Epoch 13/30 - Loss: 0.0246\n",
      "Epoch 14/30 - Loss: 0.0225\n",
      "Epoch 15/30 - Loss: 0.0209\n",
      "Epoch 16/30 - Loss: 0.0195\n",
      "Epoch 17/30 - Loss: 0.0183\n",
      "Epoch 18/30 - Loss: 0.0173\n",
      "Epoch 19/30 - Loss: 0.0164\n",
      "Epoch 20/30 - Loss: 0.0156\n",
      "Epoch 21/30 - Loss: 0.0149\n",
      "Epoch 22/30 - Loss: 0.0143\n",
      "Epoch 23/30 - Loss: 0.0137\n",
      "Epoch 24/30 - Loss: 0.0133\n",
      "Epoch 25/30 - Loss: 0.0128\n",
      "Epoch 26/30 - Loss: 0.0122\n",
      "Epoch 27/30 - Loss: 0.0116\n",
      "Epoch 28/30 - Loss: 0.0117\n",
      "Epoch 29/30 - Loss: 0.0113\n",
      "Epoch 30/30 - Loss: 0.0111\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the parameters\n",
    "input_size = 28 * 28\n",
    "hidden_size1 = 25\n",
    "hidden_size2 = 15\n",
    "output_size = 1\n",
    "\n",
    "params = {\n",
    "    \"W1\": np.random.normal(loc=0, scale=np.sqrt(2.0 / input_size), size=(input_size, hidden_size1)),\n",
    "    \"b1\": np.zeros(hidden_size1),\n",
    "    \"W2\": np.random.normal(loc=0, scale=np.sqrt(2.0 / hidden_size1), size=(hidden_size1, hidden_size2)),\n",
    "    \"b2\": np.zeros(hidden_size2),\n",
    "    \"W3\": np.random.normal(loc=0, scale=np.sqrt(2.0 / hidden_size2), size=(hidden_size2, 1)),\n",
    "    \"b3\": np.zeros(1)\n",
    "}\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Shuffle the data at the start of each epoch\n",
    "    indices = np.random.permutation(X_train.shape[0])\n",
    "    X_shuffled = X_train[indices]\n",
    "    y_shuffled = y_train[indices]\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    num_batches = X_train.shape[0] // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        activations = forward(X_batch, params)\n",
    "        predictions = activations[\"a3\"]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = binary_cross_entropy(y_batch, predictions)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Backward pass & Update Parameters\n",
    "        params = backward(X_batch, y_batch, params, activations, learning_rate)\n",
    "        \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Display the graph of the loss function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), loss_history, marker='o', color='b')\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3d317-12bb-44d8-ba2f-1203b9e8e68a",
   "metadata": {},
   "source": [
    "**Ex10. Implement the testing function.** Run the `forward` function on the images from the test set. To find the predicted class, the obtained probability is checked:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\text{if } p(y=1|x; \\theta) < 0.5 \\text{ predict class 0} \\\\\n",
    "\\text{if } p(y=1|x; \\theta) \\geq 0.5 \\text{ predict class 1}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The accuracy on the test set will also be calculated. Display some images from the test set and the predicted label.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\n",
    "$$\n",
    "\n",
    "\n",
    "> *Hint*:  $p(y=1|x; \\theta)$ represents $a_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3769c92a-3190-4d81-84cd-a2f7c9a2fc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.86%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(X_test, y_test, params, num_images_to_show=5):\n",
    "    # Forward pass on the test set\n",
    "    activations = forward(X_test, params)\n",
    "    probabilities = activations[\"a3\"]\n",
    "    \n",
    "    # Convert probabilities to class labels (0 or 1)\n",
    "    predictions = (probabilities >= 0.5).astype(int)\n",
    "    \n",
    "    # 3. Calculate Accuracy\n",
    "    correct_predictions = np.sum(predictions == y_test)\n",
    "    accuracy = correct_predictions / y_test.shape[0]\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Display sample images and their predicted labels\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i in range(num_images_to_show):\n",
    "        plt.subplot(1, num_images_to_show, i + 1)\n",
    "        \n",
    "        # Reshape the 784 flat vector back to 28*28 for display\n",
    "        img = X_test[i].reshape(28, 28)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        \n",
    "        title_color = 'green' if predictions[i] == y_test[i] else 'red'\n",
    "        plt.title(f\"Pred: {predictions[i][0]}\\nTrue: {int(y_test[i][0])}\", color=title_color)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return predictions, accuracy\n",
    "\n",
    "predictions, test_acc = evaluate_model(X_test, y_test, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcec545-9e69-4a92-9c69-0cff3d83e516",
   "metadata": {},
   "source": [
    "To better understand how neural networks work and how they learn from data, you can access the web application [playground.tensorflow.org](https://playground.tensorflow.org/) for an interactive visualization of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
